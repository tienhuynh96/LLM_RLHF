# LLM_RLHF
# InstructGPT - Reinforcement Learning with Human Feedback (RLHF)
- Step 1: Collect demonstration data, and train a supervised policy
- Step 2: Collect comparison data, and train a reward model
- Step 3: Optimize a policy against the reward model using reinforcement learning

Demo code for RLHF: [Demo]_RLHF.ipynb
Demo code for Training LLM: [Demo]-LLM-Training.ipynb
